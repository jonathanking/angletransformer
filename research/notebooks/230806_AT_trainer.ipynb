{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/net/pulsar/home/koes/jok120/openfold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/pulsar/home/koes/jok120/openfold/lib/conda/envs/openfold_venv/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "%cd ~/openfold\n",
    "\n",
    "from openfold.model.jk_sidechain_model import AngleTransformer\n",
    "from openfold.config import config\n",
    "from openfold.utils.loss import supervised_chi_loss\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(fn):\n",
    "    with open(fn, \"rb\") as f:\n",
    "        _d = pickle.load(f)\n",
    "    print(_d[\"current_datapt_number\"])\n",
    "    return _d\n",
    "\n",
    "def load_multiple_pickles(pattern):\n",
    "    files = glob.glob(pattern)\n",
    "    files.sort()\n",
    "    all_data = (load_pkl(fn) for fn in files)\n",
    "    updated_data = {}\n",
    "    starting_idx = 0\n",
    "    for fn, d in zip(files, all_data):\n",
    "        print(fn, flush=True)\n",
    "        n = d[\"current_datapt_number\"]\n",
    "        del d[\"current_datapt_number\"]\n",
    "        # Add starting index to all keys in d\n",
    "        d = {k + starting_idx: v for k, v in d.items()}\n",
    "        starting_idx += n\n",
    "        updated_data.update(d)\n",
    "\n",
    "\n",
    "    return updated_data\n",
    "\n",
    "\n",
    "BASEPATH = \"/net/pulsar/home/koes/jok120/openfold/out/experiments/angletransformer-make-caches-50-TrainSample/\"\n",
    "# %ls -hlt $BASEPATH*.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "/net/pulsar/home/koes/jok120/openfold/out/experiments/angletransformer-make-caches-50-TrainSample/angle_transformer_intermediates0_val.pkl\n",
      "47\n",
      "/net/pulsar/home/koes/jok120/openfold/out/experiments/angletransformer-make-caches-50-TrainSample/angle_transformer_intermediates1_val.pkl\n",
      "47\n",
      "/net/pulsar/home/koes/jok120/openfold/out/experiments/angletransformer-make-caches-50-TrainSample/angle_transformer_intermediates2_val.pkl\n",
      "47\n",
      "/net/pulsar/home/koes/jok120/openfold/out/experiments/angletransformer-make-caches-50-TrainSample/angle_transformer_intermediates3_val.pkl\n"
     ]
    }
   ],
   "source": [
    "d = load_multiple_pickles(os.path.join(BASEPATH, '*_val.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATModuleLit(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_dict,\n",
    "            c_s=384,\n",
    "            c_hidden=256,\n",
    "            no_blocks=2,\n",
    "            no_angles=config.model.structure_module.no_angles,  # 7\n",
    "            epsilon=config.globals.eps,\n",
    "            dropout=0.1,\n",
    "            d_ff=2048,\n",
    "            no_heads=4,\n",
    "            activation='relu',\n",
    "            batch_size=1,\n",
    "            num_workers=0):\n",
    "        super().__init__()\n",
    "        self.at = AngleTransformer(c_s=c_s,\n",
    "                                   c_hidden=c_hidden,\n",
    "                                   no_blocks=no_blocks,\n",
    "                                   no_angles=no_angles,\n",
    "                                   epsilon=epsilon,\n",
    "                                   dropout=dropout,\n",
    "                                   d_ff=d_ff,\n",
    "                                   no_heads=no_heads,\n",
    "                                   activation=activation)\n",
    "        self.loss = supervised_chi_loss\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def forward(self, s, s_initial):\n",
    "        return self.at(s, s_initial)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        s, s_initial = batch['s'][:, -1, ...].squeeze(1), batch['s_initial'].squeeze(1)\n",
    "        unnorm_ang, ang = self(s, s_initial)\n",
    "        loss = self.loss(angles_sin_cos=ang,\n",
    "                         unnormalized_angles_sin_cos=unnorm_ang,\n",
    "                         aatype=batch['aatype'],\n",
    "                         seq_mask=batch['seq_mask'],\n",
    "                         chi_mask=batch['chi_mask'],\n",
    "                         chi_angles_sin_cos=batch['chi_angles_sin_cos'],\n",
    "                         chi_weight=config.loss.supervised_chi.chi_weight,\n",
    "                         angle_norm_weight=config.loss.supervised_chi.angle_norm_weight,\n",
    "                         eps=1e-6)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(ATDataset(self.dataset_dict),\n",
    "                                           batch_size=self.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=self.num_workers,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "class ATDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        self.dataset_dict = dataset_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dict) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_dict[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if len(batch) == 1:\n",
    "        d = {\n",
    "            k: batch[0][k][0].unsqueeze(0).float()\n",
    "            for k in batch[0].keys() if k != 'name'\n",
    "        }\n",
    "        d['name'] = [batch[0]['name']]\n",
    "        d['aatype'] = batch[0]['aatype'].long()\n",
    "        return d\n",
    "    else:\n",
    "        # Needs work for padding to work\n",
    "        max_len = max([b['s'][0].shape[-2] for b in batch])\n",
    "        d = {}\n",
    "        for prot in batch:\n",
    "            for k, v in prot.items():\n",
    "                if k not in d:\n",
    "                    d[k] = []\n",
    "                v = v[0]\n",
    "                if k == 's_initial':\n",
    "                    len_diff = max_len - v.shape[-2]\n",
    "                    new_value = torch.cat([v, torch.zeros(v.shape[0], len_diff, v.shape[-1]).float()], dim=-2)\n",
    "                    d[k].append(new_value)\n",
    "                elif k != 'name':\n",
    "                    try:\n",
    "                        len_diff = max_len - v.shape[-2]\n",
    "                        new_value = torch.cat([v, torch.zeros(v.shape[0], v.shape[1], len_diff, v.shape[-1]).float()], dim=-2)\n",
    "                        d[k].append(new_value)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(k)\n",
    "                        print(v.shape)\n",
    "                        print(max_len)\n",
    "                        print(len_diff)\n",
    "                        raise e\n",
    "                else:\n",
    "                    d[k].append(prot[k][0])\n",
    "        \n",
    "        d = {\n",
    "            k: torch.stack(d[k]).float()\n",
    "            for k in d.keys() if k != 'name'\n",
    "        }\n",
    "\n",
    "        d['name'] = [b['name'] for b in batch]\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_lit = ATModuleLit(dataset_dict=d, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | at   | AngleTransformer | 2.8 M \n",
      "------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.323    Total estimated model params size (MB)\n",
      "/net/pulsar/home/koes/jok120/openfold/lib/conda/envs/openfold_venv/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/187 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/pulsar/home/koes/jok120/openfold/lib/conda/envs/openfold_venv/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/closure.py:35: LightningDeprecationWarning: One of the returned values {'angle_norm_loss', 'sq_chi_loss'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 187/187 [00:04<00:00, 44.83it/s, loss=0.358, v_num=34]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "at_lit = ATModuleLit(dataset_dict=d, batch_size=1, num_workers=1)\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=1)\n",
    "trainer.fit(at_lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(ATDataset(d),\n",
    "                                 batch_size=1,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=0,\n",
    "                                 collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['s', 's_initial', 'aatype', 'seq_mask', 'chi_mask', 'chi_angles_sin_cos', 'name'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(dl))\n",
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 162, 384])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['s'][:, -1, ...].cuda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_lit.cuda()\n",
    "uang, ang = at_lit.at(b['s'][:, -1, ...].cuda().squeeze(1), b['s_initial'].cuda().squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang.shape, uang.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openfold.np import residue_constants\n",
    "# from openfold.utils.tensor_utils import (\n",
    "#     tree_map,\n",
    "#     tensor_tree_map,\n",
    "#     masked_mean,\n",
    "#     permute_final_dims,\n",
    "#     batched_gather,\n",
    "# )\n",
    "# def supervised_chi_loss(\n",
    "#     angles_sin_cos: torch.Tensor,\n",
    "#     unnormalized_angles_sin_cos: torch.Tensor,\n",
    "#     aatype: torch.Tensor,\n",
    "#     seq_mask: torch.Tensor,\n",
    "#     chi_mask: torch.Tensor,\n",
    "#     chi_angles_sin_cos: torch.Tensor,\n",
    "#     chi_weight: float,\n",
    "#     angle_norm_weight: float,\n",
    "#     eps=1e-6,\n",
    "#     **kwargs,\n",
    "# ) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#         Implements Algorithm 27 (torsionAngleLoss)\n",
    "\n",
    "#         Args:\n",
    "#             angles_sin_cos:\n",
    "#                 [*, N, 7, 2] predicted angles\n",
    "#             unnormalized_angles_sin_cos:\n",
    "#                 The same angles, but unnormalized\n",
    "#             aatype:\n",
    "#                 [*, N] residue indices\n",
    "#             seq_mask:\n",
    "#                 [*, N] sequence mask\n",
    "#             chi_mask:\n",
    "#                 [*, N, 7] angle mask\n",
    "#             chi_angles_sin_cos:\n",
    "#                 [*, N, 7, 2] ground truth angles\n",
    "#             chi_weight:\n",
    "#                 Weight for the angle component of the loss\n",
    "#             angle_norm_weight:\n",
    "#                 Weight for the normalization component of the loss\n",
    "#         Returns:\n",
    "#             [*] loss tensor\n",
    "#     \"\"\"\n",
    "#     pred_angles = angles_sin_cos[..., 3:, :]  # [8, 1, 256, 4, 2]\n",
    "#     residue_type_one_hot = torch.nn.functional.one_hot(\n",
    "#         aatype,\n",
    "#         residue_constants.restype_num + 1,\n",
    "#     )\n",
    "#     chi_pi_periodic = torch.einsum(\n",
    "#         \"...ij,jk->ik\",\n",
    "#         residue_type_one_hot.type(angles_sin_cos.dtype),\n",
    "#         angles_sin_cos.new_tensor(residue_constants.chi_pi_periodic),\n",
    "#     )\n",
    "\n",
    "#     true_chi = chi_angles_sin_cos[None]  # [1, 1, 256, 4, 2]\n",
    "\n",
    "#     shifted_mask = (1 - 2 * chi_pi_periodic).unsqueeze(-1)\n",
    "#     true_chi_shifted = shifted_mask * true_chi\n",
    "#     sq_chi_error = torch.sum((true_chi - pred_angles) ** 2, dim=-1)\n",
    "#     sq_chi_error_shifted = torch.sum(\n",
    "#         (true_chi_shifted - pred_angles) ** 2, dim=-1\n",
    "#     )\n",
    "#     sq_chi_error = torch.minimum(sq_chi_error, sq_chi_error_shifted)\n",
    "    \n",
    "#     # The ol' switcheroo\n",
    "#     sq_chi_error = sq_chi_error.permute(\n",
    "#         *range(len(sq_chi_error.shape))[1:-2], 0, -2, -1\n",
    "#     )\n",
    "\n",
    "#     sq_chi_loss = masked_mean(\n",
    "#         chi_mask[..., None, :, :], sq_chi_error, dim=(-1, -2, -3)\n",
    "#     )\n",
    "\n",
    "#     loss = chi_weight * sq_chi_loss\n",
    "\n",
    "#     angle_norm = torch.sqrt(\n",
    "#         torch.sum(unnormalized_angles_sin_cos ** 2, dim=-1) + eps\n",
    "#     )\n",
    "#     angle_norm = angle_norm.unsqueeze(0)\n",
    "#     norm_error = torch.abs(angle_norm - 1.0)\n",
    "#     norm_error = norm_error.permute(\n",
    "#         *range(len(norm_error.shape))[1:-2], 0, -2, -1\n",
    "#     )\n",
    "#     # norm_error = norm_error.unsqueeze(0)\n",
    "#     angle_norm_loss = masked_mean(\n",
    "#         seq_mask[..., None, :, None], norm_error, dim=(-1, -2, -3)\n",
    "#     )\n",
    "\n",
    "#     loss = loss + angle_norm_weight * angle_norm_loss\n",
    "\n",
    "#     # Average over the batch dimension\n",
    "#     loss = torch.mean(loss)\n",
    "\n",
    "#     # Compute the MAE so we know exactly how good the angle prediction is in Radians\n",
    "#     # print(pred_angles.shape)\n",
    "#     # pred = torch.transpose(pred_angles.clone(), 0, 1)  # [1, 8, 256, 4, 2]\n",
    "#     # pred = pred[:, -1, :, :, :]  # [1, 1, 256, 4, 2]\n",
    "#     # pred = pred.reshape(pred.shape[0], pred.shape[-3], pred.shape[-2], pred.shape[-1])  # [1, 256, 4, 2]\n",
    "#     # true_chi2 = chi_angles_sin_cos.clone()  # [1, 256, 4, 2]\n",
    "#     # true_chi2 = inverse_trig_transform(true_chi2, 4)  # [1, 256, 4]\n",
    "#     # pred = inverse_trig_transform(pred, 4)  # [1, 256, 4]\n",
    "#     # true_chi2 = true_chi2.masked_fill_(~chi_mask.bool(), torch.nan)\n",
    "#     # pred = pred.masked_fill_(~chi_mask.bool(), torch.nan)\n",
    "#     # mae = angle_mae(true_chi2, pred)\n",
    "\n",
    "#     loss_dict = {\"loss\": loss, \"sq_chi_loss\": sq_chi_loss, \"angle_norm_loss\": angle_norm_loss}\n",
    "\n",
    "#     return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['aatype'][0].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_chi_loss(angles_sin_cos=ang,\n",
    "                         unnormalized_angles_sin_cos=uang,\n",
    "                         aatype=b['aatype'][0].cuda(),\n",
    "                         seq_mask=b['seq_mask'][0].cuda(),\n",
    "                         chi_mask=b['chi_mask'][0].cuda(),\n",
    "                         chi_angles_sin_cos=b['chi_angles_sin_cos'][0].cuda(),\n",
    "                         chi_weight=config.loss.supervised_chi.chi_weight,\n",
    "                         angle_norm_weight=config.loss.supervised_chi.angle_norm_weight,\n",
    "                         eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidechainnetv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
