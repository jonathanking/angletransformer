{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n",
      "/net/pulsar/home/koes/jok120/openfold\n",
      "-rw-r--r-- 1 jok120 koes 134M Aug  5 16:37 out/experiments/angletransformer-make-caches-30-Valmin/angle_transformer_intermediates2_val.pkl\n",
      "-rw-r--r-- 1 jok120 koes  79M Aug  5 16:37 out/experiments/angletransformer-make-caches-30-Valmin/angle_transformer_intermediates0_val.pkl\n",
      "-rw-r--r-- 1 jok120 koes  75M Aug  5 16:37 out/experiments/angletransformer-make-caches-30-Valmin/angle_transformer_intermediates1_val.pkl\n",
      "-rw-r--r-- 1 jok120 koes  52M Aug  5 16:37 out/experiments/angletransformer-make-caches-30-Valmin/angle_transformer_intermediates3_val.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "%cd ~/openfold\n",
    "# %rm out/experiments/angletransformer-make-caches-00/*.pkl\n",
    "# BASEPATH = 'out/experiments/angletransformer-make-caches-10-65kpt1/cpus/'\n",
    "BASEPATH = \"out/experiments/angletransformer-make-caches-30-Valmin/\"\n",
    "%ls -hlt $BASEPATH*.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "fn = f\"{BASEPATH}angle_transformer_intermediates{idx}_val.pkl\"\n",
    "\n",
    "with open(fn, \"rb\") as f:\n",
    "    intermediates = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(intermediates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diter = iter(intermediates.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(diter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(intermediates.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediates[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, v = next(diter)\n",
    "k, v.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 24/24 [00:18<00:00,  1.33it/s, loss=0.369, v_num=21]"
     ]
    }
   ],
   "source": [
    "d = intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['AT']['s'][0].shape, d['AT']['s_initial'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['batch'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['aatype', 'seq_mask', 'chi_mask', 'chi_angles_sin_cos', 'name']:\n",
    "    if k != 'name':\n",
    "\n",
    "        print(k, d['batch'][k][0].shape, len(d['batch'][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "162824/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8145*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['AT']['s'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(d['AT']['s']), len(d['AT']['s_initial']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things I need based on the below:\n",
    "\n",
    "For supervised_chi_loss:\n",
    "* Input\n",
    "  - AT input, aka s and s_initial \n",
    "      - (1 x 256 x 384), (1 x 256 x 384), d['inputs'][0]\n",
    "* Output\n",
    "    * batch[\"aatype\"]\n",
    "    * batch[\"seq_mask\"]\n",
    "    * batch[\"chi_mask\"]\n",
    "    * batch[\"chi_angles_sin_cos\"] (GT angles)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# AT input: s, s_initial (1 x 256 x 384), (1 x 256 x 384)\n",
    "print(list(map(lambda x: x.shape, d['inputs'][0])))\n",
    "\n",
    "# AT output: unnormalized_angles, angles (1 x 256 x 7, 2), (1 x 256 x 7, 2)\n",
    "print(list(map(lambda x: x.shape, d['outputs'][0])))\n",
    "\n",
    "# Loss_input: (out, batch)\n",
    "# out: dict: ['msa', 'pair', 'single', 'sm', 'final_atom_positions', 'final_atom_mask', \n",
    "# 'final_affine_tensor', 'lddt_logits', 'plddt', 'distogram_logits', 'masked_msa_logits', \n",
    "# 'experimentally_resolved_logits', 'violation']\n",
    "print(d['loss_inputs'][0][0]['sm'][\"angles\"].shape)\n",
    "print(d['loss_inputs'][0][1][\"chi_angles_sin_cos\"].shape)\n",
    "\n",
    "# Loss_output: (cum_loss, loss_dict)\n",
    "print(d['loss_outputs'][0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['loss_inputs'][0][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/pulsar/home/koes/jok120/openfold/lib/conda/envs/openfold_venv/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from openfold.model.jk_sidechain_model import AngleTransformer\n",
    "from openfold.config import config\n",
    "\n",
    "from openfold.utils.loss import supervised_chi_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = AngleTransformer(\n",
    "    c_s=384,\n",
    "    c_hidden=256,\n",
    "    no_blocks=2,\n",
    "    no_angles=config.model.structure_module.no_angles,  # 7\n",
    "    epsilon=config.globals.eps,\n",
    "    dropout=0.1,\n",
    "    d_ff=2048,\n",
    "    no_heads=4,\n",
    "    activation='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]['s_initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm_ang, ang = at(d[0]['s'][0].float()[-1], d[0]['s_initial'][0].float())\n",
    "aatype, seq_mask, chi_mask, chi_angles_sin_cos = d[0]['aatype'], d[0]['seq_mask'], d[0]['chi_mask'], d[0]['chi_angles_sin_cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnorm_ang.shape, ang.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension at the start of ang\n",
    "unnorm_ang2 = unnorm_ang.unsqueeze(0)\n",
    "ang2 = ang.unsqueeze(0)\n",
    "# unnorm_ang2 = unnorm_ang\n",
    "# ang2 = ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_chi_loss(angles_sin_cos=ang2,\n",
    "                    unnormalized_angles_sin_cos=unnorm_ang2,\n",
    "                    aatype=aatype,\n",
    "                    seq_mask=seq_mask,\n",
    "                    chi_mask=chi_mask,\n",
    "                    chi_angles_sin_cos=chi_angles_sin_cos,\n",
    "                    chi_weight=config.loss.supervised_chi.chi_weight,\n",
    "                    angle_norm_weight=config.loss.supervised_chi.angle_norm_weight,\n",
    "                    eps=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a super simple pytorch lightning loop to make predictions and lower the loss\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class ATModuleLit(pl.LightningModule):\n",
    "    def __init__(self, dataset_dict):\n",
    "        super().__init__()\n",
    "        self.at = self._init_angle_transformer()\n",
    "        self.loss = supervised_chi_loss\n",
    "        self.dataset_dict = dataset_dict\n",
    "\n",
    "\n",
    "    def _init_angle_transformer(self):\n",
    "        at = AngleTransformer(\n",
    "            c_s=384,\n",
    "            c_hidden=256,\n",
    "            no_blocks=2,\n",
    "            no_angles=config.model.structure_module.no_angles,  # 7\n",
    "            epsilon=config.globals.eps,\n",
    "            dropout=0.1,\n",
    "            d_ff=2048,\n",
    "            no_heads=4,\n",
    "            activation='relu')\n",
    "        return at\n",
    "\n",
    "    def forward(self, s, s_initial):\n",
    "        return self.at(s, s_initial)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        s, s_initial = batch['s'][0].float()[-1], batch['s_initial'][0].float()\n",
    "        unnorm_ang, ang = self(s, s_initial)\n",
    "        loss = self.loss(angles_sin_cos=ang.unsqueeze(0),\n",
    "                         unnormalized_angles_sin_cos=unnorm_ang.unsqueeze(0),\n",
    "                         aatype=batch['aatype'],\n",
    "                         seq_mask=batch['seq_mask'],\n",
    "                         chi_mask=batch['chi_mask'],\n",
    "                         chi_angles_sin_cos=batch['chi_angles_sin_cos'],\n",
    "                         chi_weight=config.loss.supervised_chi.chi_weight,\n",
    "                         angle_norm_weight=config.loss.supervised_chi.angle_norm_weight,\n",
    "                         eps=1e-6)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(ATDataset(self.dataset_dict),\n",
    "                                             batch_size=1,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "class ATDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        self.dataset_dict = dataset_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_dict) -1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_dict[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return batch[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | at   | AngleTransformer | 2.8 M \n",
      "------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.323    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|██████████| 24/24 [15:38<00:00, 39.09s/it, loss=0.369, v_num=21]22]\n",
      "Epoch 53: 100%|██████████| 24/24 [15:38<00:00, 39.11s/it, loss=0.369, v_num=21]22]\n",
      "Epoch 3272: 100%|██████████| 24/24 [00:00<00:00, 27.33it/s, loss=0.000969, v_num=22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3272: 100%|██████████| 24/24 [00:18<00:00,  1.33it/s, loss=0.000969, v_num=22]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "at_lit = ATModuleLit(dataset_dict=d)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10000, gpus=1)\n",
    "\n",
    "trainer.fit(at_lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step = 0\n",
    "\n",
    "at_lit.cuda()\n",
    "opt = torch.optim.Adam(at_lit.at.parameters(), lr=1e-4)\n",
    "\n",
    "for batch in iter(loader):\n",
    "    opt.zero_grad()\n",
    "    s, s_initial = batch['AT']['s'].float().cuda()[0], batch['AT']['s_initial'].float().cuda()[0]\n",
    "    unnorm_ang, ang = at_lit(s, s_initial)\n",
    "    loss_dict = supervised_chi_loss(angles_sin_cos=ang.unsqueeze(0).float().cuda(),\n",
    "                        unnormalized_angles_sin_cos=unnorm_ang.unsqueeze(0).float().cuda(),\n",
    "                        aatype=batch['aatype'][0].cuda(),\n",
    "                        seq_mask=batch['seq_mask'][0].cuda(),\n",
    "                        chi_mask=batch['chi_mask'][0].cuda(),\n",
    "                        chi_angles_sin_cos=batch['chi_angles_sin_cos'][0].float().cuda(),\n",
    "                        chi_weight=config.loss.supervised_chi.chi_weight,\n",
    "                        angle_norm_weight=config.loss.supervised_chi.angle_norm_weight,\n",
    "                        eps=1e-6)\n",
    "    loss = loss_dict['loss']\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())\n",
    "    if step > 100:\n",
    "        break\n",
    "    step += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['AT']['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sidechainnetv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
